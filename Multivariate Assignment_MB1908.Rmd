---
title: "Multivariate Analysis Assignment"
author: "Arnab Mukherjee MB1908"
date: "24 September 2019"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

First we create a 7-variate multivariate normal model (non standardized)

## Creating a Variance-Covariance matrix:
```{r }
set.seed(1908)
library('STAT')
#Using wishart because it returns a positive definite symmetric matrix
sigma = rWishart(1,7,diag(7))
sigma
dim(sigma)
# Notice that using wishart returns a tensor so we can't directly use it as the covariance matrix.
mycov = matrix(seq(1,49),7,7)
for(i in 1:7){
  for(j in 1:7){
    mycov[i,j]= sigma[i,j,1]
  }
}
mycov
dim(mycov)
```

## Generating one observations from this model:
```{r}
library('MASS')
mvrnorm(1, mu = rep(0,7), Sigma =  mycov)
```

## Contaminating Distribution
```{r}
mvrnorm(1,mu = rep(-11,7), Sigma = diag(7))
```

# Obtaining the Data 90% true and 10% Contaminated

```{r}
data = matrix(seq(700000),100000,7)
S = sample(1:100000,10000,replace = FALSE)
for(i in 1:100000){
  if(is.element(i,S) == TRUE){
    data[i,] = mvrnorm(1,mu = rep(-11,7), Sigma = diag(7))
  }
  else{
    data[i,] = mvrnorm(1, mu = rep(0,7), Sigma =  mycov)
  }
}

```

#Now lets see how our data looks:
```{r}
head(data)
```
## Now the PCA:
```{r}
data.pca = prcomp(data, center = TRUE, scale. = TRUE)
summary(data.pca)
```


##Variance Plot:
```{r}
plot(data.pca,type = "l")
```

## Plot of the Components:
```{r}
library('ggplot2')
scores <- data.frame(data, data.pca$x[,1:2]) #As we are only considering first two principal components
pc1.2 <- qplot(x=PC1, y=PC2, data=scores) +
  theme(legend.position="none")
pc1.2
```
So as it is clear from the plot of first and second principal components we can easily see that our data is turbulated in a certain small amount of part. And the cluster on the left is the cluster of outliers.

##Finding out center of the Data Cloud 
```{r}
T_1 = median(scores$PC1)
T_2 = median(scores$PC2)
T = c(T_1,T_2)
T
```

##Robust Covariance Matrix & Location Estimates for the 7 variate distribution
```{r}
library('robust')
cov_est = covRob(data,corr = FALSE)

cov_est

cov_est_PCA = covRob(scores[,8:9])
cov_est_PCA
```

##Finding out Mahalanobis Distance & Evaluating number of outliers properly detected
```{r}
library('matrixcalc')
C = matrix.inverse(cov_est_PCA$cov)
cut_off = (14.067)^0.5 #That is the value of root over chi-squared 7,0.95

correct_outlier_detection = 0
failed_to_detect = 0
false_outlier_detection = 0

for(i in 1:100000){
  y = c(scores[i,8],scores[i,9])
  z = y-T
  critical = t(z)%*%C%*%z
  
  if(is.element(i,S)==TRUE){
    if(critical> cut_off){
      correct_outlier_detection = correct_outlier_detection + 1
    }
    else{
      failed_to_detect = failed_to_detect + 1
    }
  }
  else{
    if(critical>cut_off){
      false_outlier_detection = false_outlier_detection+1
    }
  }
}

correct_outlier_detection
failed_to_detect
false_outlier_detection
```
So We see that using mahalanobish distance we correctly specified all the outliers at 95% confidence level but the downside is we have also misspecified 20361 correct observations as outliers.












